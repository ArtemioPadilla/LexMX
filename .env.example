# LexMX Environment Variables Configuration
# Copy this file to .env and fill in your API keys to enable LLM providers

# ================================
# LLM Provider API Keys
# ================================

# OpenAI (GPT-4, GPT-4 Turbo) - Recommended for complex legal analysis
# Obtain from: https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-...

# Anthropic Claude (Claude-3.5-Sonnet) - Recommended for legal reasoning
# Obtain from: https://console.anthropic.com/
ANTHROPIC_API_KEY=sk-ant-...

# Google Gemini - Cost-effective option
# Obtain from: https://makersuite.google.com/app/apikey
GOOGLE_API_KEY=...

# AWS Bedrock - Enterprise deployment option
# Obtain from AWS Console - Bedrock service
AWS_ACCESS_KEY_ID=...
AWS_SECRET_ACCESS_KEY=...
AWS_REGION=us-east-1
AWS_BEDROCK_MODEL_ID=anthropic.claude-3-5-sonnet-20241022-v2:0

# Azure OpenAI - Enterprise deployment option
# Obtain from: Azure Portal - OpenAI Service
AZURE_OPENAI_API_KEY=...
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com
AZURE_OPENAI_DEPLOYMENT_NAME=gpt-4
AZURE_OPENAI_API_VERSION=2024-02-15-preview

# Google Vertex AI - Google Cloud enterprise option
# Obtain from Google Cloud Console - Vertex AI
GOOGLE_CLOUD_PROJECT_ID=...
GOOGLE_CLOUD_LOCATION=us-central1
VERTEX_AI_MODEL_ID=gemini-1.5-pro

# Ollama - Local LLM deployment
# Set up local Ollama server: https://ollama.ai/
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.1:8b

# ================================
# Application Configuration
# ================================

# Development/Production mode
NODE_ENV=development

# Default language (es or en)
DEFAULT_LANGUAGE=es

# Default LLM provider selection algorithm
# Options: cost_optimized, quality_first, balanced, speed_first
LLM_SELECTION_STRATEGY=balanced

# Maximum cost per query (in USD cents)
MAX_COST_PER_QUERY=10

# Enable/disable specific providers
ENABLE_OPENAI=true
ENABLE_ANTHROPIC=true
ENABLE_GOOGLE=true
ENABLE_AWS_BEDROCK=false
ENABLE_AZURE_OPENAI=false
ENABLE_VERTEX_AI=false
ENABLE_OLLAMA=false

# ================================
# RAG Engine Configuration
# ================================

# Embedding provider (openai, transformers, mock)
EMBEDDING_PROVIDER=openai

# Vector search configuration
MAX_SEARCH_RESULTS=20
SIMILARITY_THRESHOLD=0.7

# Chunking strategy
CHUNKING_STRATEGY=contextual
CHUNK_SIZE=1024
CHUNK_OVERLAP=128

# ================================
# Security & Privacy
# ================================

# Token encryption settings
TOKEN_ENCRYPTION_ENABLED=true
TOKEN_STORAGE_TYPE=sessionStorage

# Cache settings
ENABLE_QUERY_CACHE=true
CACHE_TTL_MINUTES=60

# ================================
# Monitoring & Analytics
# ================================

# Performance monitoring
ENABLE_PERFORMANCE_METRICS=true
LOG_LEVEL=info

# Cost tracking
ENABLE_COST_TRACKING=true
COST_ALERT_THRESHOLD=100

# ================================
# Legal Corpus Configuration
# ================================

# Enable/disable specific legal areas
ENABLE_CONSTITUTIONAL_LAW=true
ENABLE_LABOR_LAW=true
ENABLE_CIVIL_LAW=true
ENABLE_CRIMINAL_LAW=true
ENABLE_TAX_LAW=true
ENABLE_COMMERCIAL_LAW=true
ENABLE_ADMINISTRATIVE_LAW=true

# Corpus update settings
AUTO_UPDATE_CORPUS=false
CORPUS_VERSION_CHECK_INTERVAL=86400

# ================================
# Development Settings
# ================================

# Enable debug mode for detailed logging
DEBUG=false

# Mock providers for development/testing
USE_MOCK_PROVIDERS=false

# Enable experimental features
ENABLE_EXPERIMENTAL_FEATURES=false

# Test configuration
TEST_TIMEOUT=30000
E2E_HEADLESS=true

# ================================
# Deployment Configuration
# ================================

# GitHub Pages deployment
# These are typically set by GitHub Actions
GITHUB_REPOSITORY=your-username/LexMX
GITHUB_TOKEN=ghp_...

# Custom domain (optional)
CUSTOM_DOMAIN=lexmx.example.com

# CDN configuration (optional)
CDN_BASE_URL=https://cdn.example.com/lexmx

# ================================
# Additional Provider Settings
# ================================

# WebLLM (Browser-based local inference)
WEBLLM_CACHE_SIZE=4096
WEBLLM_MAX_TOKENS=2048

# Advanced provider configurations
OPENAI_ORGANIZATION_ID=org-...
OPENAI_MAX_TOKENS=4096
ANTHROPIC_MAX_TOKENS=4096
GOOGLE_MAX_TOKENS=2048

# Rate limiting
RATE_LIMIT_REQUESTS_PER_MINUTE=60
RATE_LIMIT_TOKENS_PER_MINUTE=10000